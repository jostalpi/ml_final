


#importar librerias 
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
import warnings  
warnings.filterwarnings('ignore')


#cargar dataset
df=pd.read_csv('../Datos/ObesityDataSet_raw_and_data_sinthetic.csv')
#valida primeros registros del dataframe
df.head()


#informacion Df
df.info()


#cantidad de filas y columnas
df.shape


#variables categoricas
categoricas=df.select_dtypes(include=['object']).columns.tolist()
print('Variables cualitativas:', categoricas)

#variables numericas
numericas=df.select_dtypes(include=['float64']).columns.tolist()
print('Variables numericas:', numericas)


#valida estadisticos
df.describe()


#histograma
df.hist(figsize=(15,10), bins=20)
plt.show()


sns.pairplot(df, height=2)
plt.show()


# Calcular la matriz de correlación
correlation_matrix = df[numericas].corr()

# Configurar el tamaño de la figura
plt.figure(figsize=(14, 10))

# Crear un mapa de calor para la matriz de correlación
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)

# Mostrar el gráfico
plt.title("Matriz de Correlación de las Variables del Dataset")
plt.show()



# Box plot
plt.figure(figsize=(15,35))
plt.subplot(4,2,1)
plt.boxplot(df.Age)
plt.title('Edad')
plt.subplot(4,2,2)
plt.boxplot(df.Height)
plt.title('Altura')
plt.subplot(4,2,3)
plt.boxplot(df.Weight)
plt.title('Peso')
plt.subplot(4,2,4)
plt.boxplot(df.FCVC)
plt.title('Frecuencia de consumo de vegetales')
plt.subplot(4,2,5)
plt.boxplot(df.NCP)
plt.title('Número de comidas principales')
plt.subplot(4,2,6)
plt.boxplot(df.CH2O)
plt.title('Consumo diario de agua')
plt.subplot(4,2,7)
plt.boxplot(df.FAF)
plt.title('Frecuencia de actividad física')
plt.subplot(4,2,8)
plt.boxplot(df.TUE)
plt.title('Tiempo usando dispositivos tecnológicos')
plt.show()


#Frecuencia variable objetivo
obesidad_counts=df['NObeyesdad'].value_counts()
obesidad_counts


# Gráfico tipo pie
plt.pie(obesidad_counts, 
        labels=obesidad_counts.index, 
        autopct='%1.1f%%')
plt.title('Distribucion Obesidad')
plt.show()


selected_columns = ['Gender','Age','family_history_with_overweight','FAVC','FCVC','NCP','CAEC','SMOKE','CH2O','SCC','FAF','TUE','CALC','MTRANS','NObeyesdad']
procesamiento = df[selected_columns].copy()

label_encoder = LabelEncoder()
for column in categoricas:
    procesamiento[column] = label_encoder.fit_transform(procesamiento[column])

procesamiento.head()


X=procesamiento.drop(['NObeyesdad'], axis=1)
y=df['NObeyesdad']

scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

X_scaled = pd.DataFrame(X_scaled, columns=X.columns)
X_scaled.head()





# Choosing the Number of Clusters (k)
# Using the Elbow Method to find the optimal number of clusters
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_)

# Plot the results of the Elbow method
plt.plot(range(1, 11), wcss)
plt.title('Método del Codo')
plt.xlabel('Clusters')
plt.ylabel('WCSS') # Within cluster sum of squares
plt.show()


# numero de clusters
optimal_k = 3

# entrena modeloa
kmeans = KMeans(n_clusters=optimal_k, init='k-means++', max_iter=300, n_init=10, random_state=0)
clusters = kmeans.fit_predict(X_scaled)

cluster_labels = kmeans.labels_

df['Cluster'] = cluster_labels


df.head()


from sklearn.decomposition import PCA

# inicializa PCA
pca = PCA(n_components=3)
df_pca = pca.fit_transform(X_scaled)
centroids = kmeans.cluster_centers_
centroids_pca = pca.transform(centroids)

# PCA inicializacion y transformacion
plt.figure(figsize=(10, 7))
plt.scatter(df_pca[:, 0], df_pca[:, 1], c=clusters, cmap='cividis', marker='o', label='Puntos de datos')
plt.scatter(centroids_pca[:, 0], centroids_pca[:, 1], c='red', marker='x', s=200, label='Centroides')
plt.title('2D PCA Visualizacion de Clusters con Centroides')
plt.xlabel('PCA1')
plt.ylabel('PCA2')
plt.legend()
plt.show()


# Definir k para clustering
k = 3

# Clustering K-Means
kmeans = KMeans(n_clusters=k, random_state=42)
labels_kmeans = kmeans.fit_predict(X_scaled)
silhouette_kmeans = silhouette_score(X_scaled, labels_kmeans)
print(f"Coeficiente de Silueta para K-Means: {silhouette_kmeans:.4f}")

# Clustering Jerárquico
agg = AgglomerativeClustering(n_clusters=k, linkage='ward')
labels_agg = agg.fit_predict(X_scaled)
silhouette_agg = silhouette_score(X_scaled, labels_agg)
print(f"Coeficiente de Silueta para Clustering Jerárquico: {silhouette_agg:.4f}")

# DBSCAN
dbscan = DBSCAN(eps=2, min_samples=5)
labels_dbscan = dbscan.fit_predict(X_scaled)
mask = labels_dbscan != -1
if len(set(labels_dbscan)) > 1:
    silhouette_dbscan = silhouette_score(X_scaled[mask], labels_dbscan[mask])
    print(f"Coeficiente de Silueta para DBSCAN: {silhouette_dbscan:.4f}")
else:
    print("DBSCAN no encontró clústeres distintos.")

# Modelo de Mezcla Gaussiana
gmm = GaussianMixture(n_components=k, covariance_type='full', random_state=42)
labels_gmm = gmm.fit_predict(X_scaled)
silhouette_gmm = silhouette_score(X_scaled, labels_gmm)
print(f"Coeficiente de Silueta para GMM: {silhouette_gmm:.4f}")


## Visualización de los resultados

import matplotlib.pyplot as plt
import seaborn as sns

# Crear un DataFrame con los resultados
results = pd.DataFrame({
    'K-Means': labels_kmeans,
    'Jerárquico': labels_agg,
    'DBSCAN': labels_dbscan,
    'GMM': labels_gmm
})

# Visualizar los resultados
## Hagamos un Tsne para visualizar los resultados

from sklearn.manifold import TSNE
tsne=TSNE(n_components=3, random_state=42)
X_tsne=tsne.fit_transform(X_scaled)

results['TSNE1'] = X_tsne[:,0]
results['TSNE2'] = X_tsne[:,1]

plt.figure(figsize=(12, 4))

plt.subplot(1, 4, 1)
sns.scatterplot(x='TSNE1', y='TSNE2', hue='K-Means', palette='viridis', legend=False, data=results)
plt.title('K-Means')

plt.subplot(1, 4, 2)
sns.scatterplot(x='TSNE1', y='TSNE2', hue='Jerárquico', palette='viridis', legend=False, data=results)
plt.title('Jerárquico')

plt.subplot(1, 4, 3)
sns.scatterplot(x='TSNE1', y='TSNE2', hue='DBSCAN', palette='viridis', legend=False, data=results)
plt.title('DBSCAN')

plt.subplot(1, 4, 4)
sns.scatterplot(x='TSNE1', y='TSNE2', hue='GMM', palette='viridis', legend=False, data=results)
plt.title('GMM')

plt.tight_layout()
plt.show()


## Visualización de los Coeficientes de Silueta
from sklearn.metrics import silhouette_score

plt.figure(figsize=(12, 4))

plt.subplot(1, 4, 1)
sns.lineplot(x=['K-Means', 'Jerárquico', 'GMM'], y=[silhouette_kmeans, silhouette_agg, silhouette_gmm])

plt.title('Coeficiente de Silueta')

plt.tight_layout()
plt.show()


## Visualización de los resultados

import matplotlib.pyplot as plt
import seaborn as sns

# Crear un DataFrame con los resultados
results = pd.DataFrame({
    'K-Means': labels_kmeans,
    'Jerárquico': labels_agg,
    'GMM': labels_gmm
})

# Visualizar los resultados
## Hagamos un PCA para visualizar los resultados

from sklearn.decomposition import PCA

pca = PCA(n_components=3)
X_pca = pca.fit_transform(X_scaled)

results['PCA1'] = X_pca[:,0]
results['PCA2'] = X_pca[:,1]

plt.figure(figsize=(12, 4))

plt.subplot(1, 4, 1)
sns.scatterplot(x='PCA1', y='PCA2', hue='K-Means', palette='viridis',  data=results)
plt.title('K-Means')

plt.subplot(1, 4, 2)
sns.scatterplot(x='PCA1', y='PCA2', hue='Jerárquico', palette='viridis',  data=results)
plt.title('Jerárquico')

plt.subplot(1, 4, 3)
sns.scatterplot(x='PCA1', y='PCA2', hue='GMM', palette='viridis',  data=results)
plt.title('GMM')

plt.tight_layout()
plt.show()


## Comparemos PCA con Target

results['target'] = y

plt.figure(figsize=(15, 8))

plt.subplot(1, 4, 1)
sns.scatterplot(x='PCA1', y='PCA2', hue='target', palette='viridis',  data=results)
plt.title('Target')


## Usemos un boxplot para comparar PCA con Target

plt.figure(figsize=(20, 5))

plt.subplot(1, 2, 1)
sns.boxplot(x='target', y='PCA1', data=results)

plt.subplot(1, 2, 2)
sns.boxplot(x='target', y='PCA2', data=results)

plt.tight_layout()
plt.show()





obesidad=df['NObeyesdad'].value_counts()
print(obesidad)


#preprocesamiento
categorical_columns = ['Gender', 'family_history_with_overweight', 'FAVC', 'CAEC', 'SMOKE', 'SCC', 'CALC', 'MTRANS']

pre1 = df.copy()
pre1.drop('NObeyesdad', axis=1, inplace=True)

pre1.head()


label_encoder = LabelEncoder()
for column in categorical_columns:
    pre1[column] = label_encoder.fit_transform(pre1[column])

pre1.head()


#preprocesamiento
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

X = pre1
y = df['NObeyesdad']
n_classes = len(set(y))

## Dividir el dataset en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)


#RandomForest
import warnings
warnings.filterwarnings('ignore')
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# Entrenamos el modelo
modelo_rf = RandomForestClassifier(n_estimators=100, random_state=42)
modelo_rf.fit(X_train, y_train)

## Predicción
y_pred = modelo_rf.predict(X_test)

## Evaluación
print(classification_report(y_test, y_pred))


from sklearn.metrics import PrecisionRecallDisplay
from sklearn.multiclass import OneVsRestClassifier
from sklearn.linear_model import LogisticRegression
import matplotlib.pyplot as plt

# Entrenar el clasificador One-vs-Rest con Logistic Regression (o el modelo de tu elección)
modelo_ovr = OneVsRestClassifier(RandomForestClassifier())
modelo_ovr.fit(X_train, y_train)

# Generar la curva de precisión-recall para cada clase
fig, ax = plt.subplots(figsize=(10, 8))
for i, class_label in enumerate(modelo_ovr.classes_):
    PrecisionRecallDisplay.from_estimator(
        modelo_ovr.estimators_[i],
        X_test,
        (y_test == class_label).astype(int),  # Etiquetar binariamente cada clase
        name=f"Clase {class_label}",
        ax=ax
    )

plt.title("Curvas de Precisión-Recall para cada clase")
plt.legend()
plt.show()



## Regresón Logistica
import warnings
warnings.filterwarnings('ignore')
from sklearn.linear_model import LogisticRegression

modelo_lr = LogisticRegression(random_state=42)
modelo_lr.fit(X_train, y_train)

## Predicción
y_pred = modelo_lr.predict(X_test)

## Evaluación
print(classification_report(y_test, y_pred))


from sklearn.metrics import PrecisionRecallDisplay
from sklearn.multiclass import OneVsRestClassifier
from sklearn.linear_model import LogisticRegression
import matplotlib.pyplot as plt

# Entrenar el clasificador One-vs-Rest con Logistic Regression (o el modelo de tu elección)
modelo_ovr = OneVsRestClassifier(LogisticRegression())
modelo_ovr.fit(X_train, y_train)

# Generar la curva de precisión-recall para cada clase
fig, ax = plt.subplots(figsize=(10, 8))
for i, class_label in enumerate(modelo_ovr.classes_):
    PrecisionRecallDisplay.from_estimator(
        modelo_ovr.estimators_[i],
        X_test,
        (y_test == class_label).astype(int),  # Etiquetar binariamente cada clase
        name=f"Clase {class_label}",
        ax=ax
    )

plt.title("Curvas de Precisión-Recall para cada clase")
plt.legend()
plt.show()



from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_pred, y_test)
sns.heatmap(cm,annot=True)
plt.show()


## KNN
import warnings
warnings.filterwarnings('ignore')
from sklearn.neighbors import KNeighborsClassifier

modelo_knn = KNeighborsClassifier(n_neighbors=5)
modelo_knn.fit(X_train, y_train)

## Predicción
y_pred = modelo_knn.predict(X_test)

## Evaluación
print(classification_report(y_test, y_pred))


## Clasificación con SVM
import warnings
warnings.filterwarnings('ignore')
from sklearn.svm import SVC
from sklearn.metrics import classification_report

modelo_svc = SVC(kernel='linear', C=1.0, random_state=0)
modelo_svc.fit(X_train, y_train)

## Predicción
y_pred = modelo_svc.predict(X_test)

## Evaluación
print(classification_report(y_test, y_pred))





from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from scipy.stats import randint
import pandas as pd

# Paso 1: Preparar los datos
X = pre1
y = df['NObeyesdad']  # Variable objetivo

# Dividir los datos en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Paso 2: Definir el modelo
rf = RandomForestClassifier(random_state=42)

# Paso 3: Configurar el espacio de búsqueda de hiperparámetros
# Para GridSearchCV
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

# Paso 4: Ejecutar la búsqueda de hiperparámetros

# Grid Search
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)
grid_search.fit(X_train, y_train)
print("Mejores parámetros (Grid Search):", grid_search.best_params_)

# Paso 5: Evaluar el modelo con los mejores hiperparámetros en el conjunto de prueba
best_rf_grid = grid_search.best_estimator_

# Predicciones y evaluación
y_pred_grid = best_rf_grid.predict(X_test)

print("Rendimiento de Grid Search:")
print(classification_report(y_test, y_pred_grid))




from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import classification_report
import pandas as pd

# Paso 1: Preparar los datos
X = pre1
y = df['NObeyesdad']  # Variable objetivo

# Dividir los datos en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Paso 2: Definir el modelo de regresión logística
log_reg = LogisticRegression(random_state=42)

# Paso 3: Configurar el espacio de búsqueda de hiperparámetros
param_grid = {
    'C': [0.01, 0.1, 1, 10, 100],              # Valores de regularización
    'penalty': ['l2', 'none'],                  # Tipos de penalización
    'solver': ['lbfgs', 'liblinear', 'saga']    # Algoritmos de optimización compatibles
}

# Paso 4: Ejecutar GridSearchCV
grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Imprimir los mejores hiperparámetros encontrados
print("Mejores parámetros encontrados:", grid_search.best_params_)

# Paso 5: Evaluar el modelo con los mejores hiperparámetros
best_log_reg = grid_search.best_estimator_
y_pred = best_log_reg.predict(X_test)

# Reporte de clasificación
print("Reporte de clasificación para el modelo optimizado:")
print(classification_report(y_test, y_pred))




